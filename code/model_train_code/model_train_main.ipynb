{"cells":[{"cell_type":"code","execution_count":1,"id":"71bc1484-acde-460d-84b5-71053f282877","metadata":{},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.types import StringType, DateType, FloatType, IntegerType, TimestampType, ArrayType, StructType, StructField\n","from pyspark.sql.functions import from_unixtime, sum, rank,lag, explode, expr,spark_partition_id, to_date, coalesce, lit, to_timestamp, col, month, concat, count, max, when, dayofweek, datediff,dense_rank, desc, date_format\n","import pyspark.sql.functions as F\n","from pyspark.sql.window import Window\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import re\n","from pyspark.ml.feature import Tokenizer, StopWordsRemover, Word2Vec,HashingTF,IDF, CountVectorizer,VectorAssembler\n","from pyspark.sql.functions import udf\n","from pyspark.ml.feature import Tokenizer, CountVectorizer, IDF\n","from pyspark.ml import Pipeline,PipelineModel\n","from sparknlp.base import DocumentAssembler, Finisher\n","from sparknlp.annotator import LemmatizerModel\n","from pyspark.ml.classification import LinearSVC, LogisticRegression\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator,BinaryClassificationEvaluator\n","from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n","import sparknlp\n","import warnings\n","from google.cloud import storage"]},{"cell_type":"code","execution_count":2,"id":"8ece1179-fc2a-4b12-93b6-0910fa154652","metadata":{},"outputs":[],"source":["# # remove all warnings\n","# warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":3,"id":"b08a2971-7bc1-4bd6-ab96-6837bd3569d3","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[":: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"]},{"name":"stderr","output_type":"stream","text":["Ivy Default Cache set to: /root/.ivy2/cache\n","The jars for the packages stored in: /root/.ivy2/jars\n","com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n","graphframes#graphframes added as a dependency\n",":: resolving dependencies :: org.apache.spark#spark-submit-parent-8eb4bcf7-cecc-4e71-9be9-59df7f016802;1.0\n","\tconfs: [default]\n","\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;4.4.0 in central\n","\tfound com.typesafe#config;1.4.2 in central\n","\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n","\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n","\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n","\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n","\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n","\tfound com.google.code.gson#gson;2.3 in central\n","\tfound it.unimi.dsi#fastutil;7.0.12 in central\n","\tfound org.projectlombok#lombok;1.16.8 in central\n","\tfound com.google.cloud#google-cloud-storage;2.16.0 in central\n","\tfound com.google.guava#guava;31.1-jre in central\n","\tfound com.google.guava#failureaccess;1.0.1 in central\n","\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n","\tfound com.google.errorprone#error_prone_annotations;2.16 in central\n","\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n","\tfound com.google.http-client#google-http-client;1.42.3 in central\n","\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n","\tfound com.google.http-client#google-http-client-jackson2;1.42.3 in central\n","\tfound com.google.http-client#google-http-client-gson;1.42.3 in central\n","\tfound com.google.api-client#google-api-client;2.1.1 in central\n","\tfound commons-codec#commons-codec;1.15 in central\n","\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n","\tfound com.google.http-client#google-http-client-apache-v2;1.42.3 in central\n","\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n","\tfound com.google.code.gson#gson;2.10 in central\n","\tfound com.google.cloud#google-cloud-core;2.9.0 in central\n","\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n","\tfound com.google.cloud#google-cloud-core-http;2.9.0 in central\n","\tfound com.google.http-client#google-http-client-appengine;1.42.3 in central\n","\tfound com.google.api#gax-httpjson;0.105.1 in central\n","\tfound com.google.cloud#google-cloud-core-grpc;2.9.0 in central\n","\tfound io.grpc#grpc-core;1.51.0 in central\n","\tfound com.google.api#gax;2.20.1 in central\n","\tfound com.google.api#gax-grpc;2.20.1 in central\n","\tfound io.grpc#grpc-alts;1.51.0 in central\n","\tfound io.grpc#grpc-grpclb;1.51.0 in central\n","\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n","\tfound io.grpc#grpc-protobuf;1.51.0 in central\n","\tfound com.google.auth#google-auth-library-credentials;1.13.0 in central\n","\tfound com.google.auth#google-auth-library-oauth2-http;1.13.0 in central\n","\tfound com.google.api#api-common;2.2.2 in central\n","\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n","\tfound io.opencensus#opencensus-api;0.31.1 in central\n","\tfound io.grpc#grpc-context;1.51.0 in central\n","\tfound com.google.api.grpc#proto-google-iam-v1;1.6.22 in central\n","\tfound com.google.protobuf#protobuf-java;3.21.10 in central\n","\tfound com.google.protobuf#protobuf-java-util;3.21.10 in central\n","\tfound com.google.api.grpc#proto-google-common-protos;2.11.0 in central\n","\tfound org.threeten#threetenbp;1.6.4 in central\n","\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.16.0-alpha in central\n","\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.16.0-alpha in central\n","\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.16.0-alpha in central\n","\tfound com.fasterxml.jackson.core#jackson-core;2.14.1 in central\n","\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n","\tfound io.grpc#grpc-api;1.51.0 in central\n","\tfound io.grpc#grpc-auth;1.51.0 in central\n","\tfound io.grpc#grpc-stub;1.51.0 in central\n","\tfound org.checkerframework#checker-qual;3.28.0 in central\n","\tfound com.google.api.grpc#grpc-google-iam-v1;1.6.22 in central\n","\tfound io.grpc#grpc-protobuf-lite;1.51.0 in central\n","\tfound com.google.android#annotations;4.1.1.4 in central\n","\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n","\tfound io.grpc#grpc-netty-shaded;1.51.0 in central\n","\tfound io.perfmark#perfmark-api;0.26.0 in central\n","\tfound io.grpc#grpc-googleapis;1.51.0 in central\n","\tfound io.grpc#grpc-xds;1.51.0 in central\n","\tfound io.opencensus#opencensus-proto;0.2.0 in central\n","\tfound io.grpc#grpc-services;1.51.0 in central\n","\tfound com.google.re2j#re2j;1.6 in central\n","\tfound com.navigamez#greex;1.0 in central\n","\tfound dk.brics.automaton#automaton;1.11-8 in central\n","\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n","\tfound graphframes#graphframes;0.8.2-spark3.1-s_2.12 in spark-packages\n","\tfound org.slf4j#slf4j-api;1.7.16 in central\n",":: resolution report :: resolve 1183ms :: artifacts dl 48ms\n","\t:: modules in use:\n","\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n","\tcom.fasterxml.jackson.core#jackson-core;2.14.1 from central in [default]\n","\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n","\tcom.google.android#annotations;4.1.1.4 from central in [default]\n","\tcom.google.api#api-common;2.2.2 from central in [default]\n","\tcom.google.api#gax;2.20.1 from central in [default]\n","\tcom.google.api#gax-grpc;2.20.1 from central in [default]\n","\tcom.google.api#gax-httpjson;0.105.1 from central in [default]\n","\tcom.google.api-client#google-api-client;2.1.1 from central in [default]\n","\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.16.0-alpha from central in [default]\n","\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.16.0-alpha from central in [default]\n","\tcom.google.api.grpc#grpc-google-iam-v1;1.6.22 from central in [default]\n","\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.16.0-alpha from central in [default]\n","\tcom.google.api.grpc#proto-google-common-protos;2.11.0 from central in [default]\n","\tcom.google.api.grpc#proto-google-iam-v1;1.6.22 from central in [default]\n","\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n","\tcom.google.auth#google-auth-library-credentials;1.13.0 from central in [default]\n","\tcom.google.auth#google-auth-library-oauth2-http;1.13.0 from central in [default]\n","\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n","\tcom.google.cloud#google-cloud-core;2.9.0 from central in [default]\n","\tcom.google.cloud#google-cloud-core-grpc;2.9.0 from central in [default]\n","\tcom.google.cloud#google-cloud-core-http;2.9.0 from central in [default]\n","\tcom.google.cloud#google-cloud-storage;2.16.0 from central in [default]\n","\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n","\tcom.google.code.gson#gson;2.10 from central in [default]\n","\tcom.google.errorprone#error_prone_annotations;2.16 from central in [default]\n","\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n","\tcom.google.guava#guava;31.1-jre from central in [default]\n","\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n","\tcom.google.http-client#google-http-client;1.42.3 from central in [default]\n","\tcom.google.http-client#google-http-client-apache-v2;1.42.3 from central in [default]\n","\tcom.google.http-client#google-http-client-appengine;1.42.3 from central in [default]\n","\tcom.google.http-client#google-http-client-gson;1.42.3 from central in [default]\n","\tcom.google.http-client#google-http-client-jackson2;1.42.3 from central in [default]\n","\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n","\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n","\tcom.google.protobuf#protobuf-java;3.21.10 from central in [default]\n","\tcom.google.protobuf#protobuf-java-util;3.21.10 from central in [default]\n","\tcom.google.re2j#re2j;1.6 from central in [default]\n","\tcom.johnsnowlabs.nlp#spark-nlp_2.12;4.4.0 from central in [default]\n","\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n","\tcom.navigamez#greex;1.0 from central in [default]\n","\tcom.typesafe#config;1.4.2 from central in [default]\n","\tcommons-codec#commons-codec;1.15 from central in [default]\n","\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n","\tgraphframes#graphframes;0.8.2-spark3.1-s_2.12 from spark-packages in [default]\n","\tio.grpc#grpc-alts;1.51.0 from central in [default]\n","\tio.grpc#grpc-api;1.51.0 from central in [default]\n","\tio.grpc#grpc-auth;1.51.0 from central in [default]\n","\tio.grpc#grpc-context;1.51.0 from central in [default]\n","\tio.grpc#grpc-core;1.51.0 from central in [default]\n","\tio.grpc#grpc-googleapis;1.51.0 from central in [default]\n","\tio.grpc#grpc-grpclb;1.51.0 from central in [default]\n","\tio.grpc#grpc-netty-shaded;1.51.0 from central in [default]\n","\tio.grpc#grpc-protobuf;1.51.0 from central in [default]\n","\tio.grpc#grpc-protobuf-lite;1.51.0 from central in [default]\n","\tio.grpc#grpc-services;1.51.0 from central in [default]\n","\tio.grpc#grpc-stub;1.51.0 from central in [default]\n","\tio.grpc#grpc-xds;1.51.0 from central in [default]\n","\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n","\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n","\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n","\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n","\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n","\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n","\torg.checkerframework#checker-qual;3.28.0 from central in [default]\n","\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n","\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n","\torg.projectlombok#lombok;1.16.8 from central in [default]\n","\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n","\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n","\torg.threeten#threetenbp;1.6.4 from central in [default]\n","\t:: evicted modules:\n","\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.10] in [default]\n","\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.10] in [default]\n","\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10] in [default]\n","\t---------------------------------------------------------------------\n","\t|                  |            modules            ||   artifacts   |\n","\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n","\t---------------------------------------------------------------------\n","\t|      default     |   75  |   0   |   0   |   3   ||   72  |   0   |\n","\t---------------------------------------------------------------------\n",":: retrieving :: org.apache.spark#spark-submit-parent-8eb4bcf7-cecc-4e71-9be9-59df7f016802\n","\tconfs: [default]\n","\t0 artifacts copied, 72 already retrieved (0kB/23ms)\n","Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","23/11/27 04:06:50 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n","23/11/27 04:06:50 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n","23/11/27 04:06:50 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n","23/11/27 04:06:50 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.johnsnowlabs.nlp_spark-nlp_2.12-4.4.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/graphframes_graphframes-0.8.2-spark3.1-s_2.12.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.typesafe_config-1.4.2.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.rocksdb_rocksdbjni-6.29.5.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.828.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.github.universal-automata_liblevenshtein-3.0.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.cloud_google-cloud-storage-2.16.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.navigamez_greex-1.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.johnsnowlabs.nlp_tensorflow-cpu_2.12-0.4.4.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/it.unimi.dsi_fastutil-7.0.12.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.projectlombok_lombok-1.16.8.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.guava_guava-31.1-jre.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.guava_failureaccess-1.0.1.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.errorprone_error_prone_annotations-2.16.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.j2objc_j2objc-annotations-1.3.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.http-client_google-http-client-1.42.3.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.opencensus_opencensus-contrib-http-util-0.31.1.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.http-client_google-http-client-jackson2-1.42.3.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.http-client_google-http-client-gson-1.42.3.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.api-client_google-api-client-2.1.1.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/commons-codec_commons-codec-1.15.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.oauth-client_google-oauth-client-1.34.1.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.http-client_google-http-client-apache-v2-1.42.3.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.apis_google-api-services-storage-v1-rev20220705-2.0.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.code.gson_gson-2.10.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.cloud_google-cloud-core-2.9.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.auto.value_auto-value-annotations-1.10.1.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.cloud_google-cloud-core-http-2.9.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.http-client_google-http-client-appengine-1.42.3.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.api_gax-httpjson-0.105.1.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.cloud_google-cloud-core-grpc-2.9.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-core-1.51.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.api_gax-2.20.1.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.api_gax-grpc-2.20.1.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-alts-1.51.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-grpclb-1.51.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.conscrypt_conscrypt-openjdk-uber-2.5.2.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-protobuf-1.51.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.auth_google-auth-library-credentials-1.13.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.auth_google-auth-library-oauth2-http-1.13.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.api_api-common-2.2.2.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/javax.annotation_javax.annotation-api-1.3.2.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.opencensus_opencensus-api-0.31.1.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-context-1.51.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.api.grpc_proto-google-iam-v1-1.6.22.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.protobuf_protobuf-java-3.21.10.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.protobuf_protobuf-java-util-3.21.10.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.api.grpc_proto-google-common-protos-2.11.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.threeten_threetenbp-1.6.4.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.api.grpc_proto-google-cloud-storage-v2-2.16.0-alpha.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.api.grpc_grpc-google-cloud-storage-v2-2.16.0-alpha.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.api.grpc_gapic-google-cloud-storage-v2-2.16.0-alpha.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.14.1.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-api-1.51.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-auth-1.51.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-stub-1.51.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.checkerframework_checker-qual-3.28.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.api.grpc_grpc-google-iam-v1-1.6.22.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-protobuf-lite-1.51.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.android_annotations-4.1.1.4.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.codehaus.mojo_animal-sniffer-annotations-1.22.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-netty-shaded-1.51.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.perfmark_perfmark-api-0.26.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-googleapis-1.51.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-xds-1.51.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.opencensus_opencensus-proto-0.2.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-services-1.51.0.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.re2j_re2j-1.6.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/dk.brics.automaton_automaton-1.11-8.jar added multiple times to distributed cache.\n","23/11/27 04:06:54 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar added multiple times to distributed cache.\n"]}],"source":["# spark = SparkSession.builder.appName('SparkBasics').getOrCreate()\n","spark = SparkSession.builder.appName('ml').getOrCreate()\n","stoarge_client = storage.Client()\n","# Get the context of the Pyspark environment\n","spark.sparkContext.getConf().getAll()\n","# Store spark context as a variable\n","sc = spark.sparkContext"]},{"cell_type":"code","execution_count":4,"id":"435cc3d3-c010-4441-8f3f-b7065e43971a","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["root\n"," |-- archived: string (nullable = true)\n"," |-- author: string (nullable = true)\n"," |-- author_fullname: string (nullable = true)\n"," |-- body: string (nullable = true)\n"," |-- comment_type: string (nullable = true)\n"," |-- controversiality: string (nullable = true)\n"," |-- created_utc: string (nullable = true)\n"," |-- edited: string (nullable = true)\n"," |-- gilded: string (nullable = true)\n"," |-- id: string (nullable = true)\n"," |-- link_id: string (nullable = true)\n"," |-- locked: string (nullable = true)\n"," |-- name: string (nullable = true)\n"," |-- parent_id: string (nullable = true)\n"," |-- permalink: string (nullable = true)\n"," |-- retrieved_on: string (nullable = true)\n"," |-- score: string (nullable = true)\n"," |-- subreddit_id: string (nullable = true)\n"," |-- subreddit_name_prefixed: string (nullable = true)\n"," |-- subreddit_type: string (nullable = true)\n"," |-- total_awards_received: string (nullable = true)\n","\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 2:============================================>              (3 + 1) / 4]\r"]},{"name":"stdout","output_type":"stream","text":["+-----------------------+\n","|subreddit_name_prefixed|\n","+-----------------------+\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","+-----------------------+\n","only showing top 20 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["reddit_data_df = spark.read.parquet(\"gs://msca-bdp-student-gcs/Group2_Final_Project/reddit_data/\",header=True, inferSchema=True)\n","reddit_data_df = reddit_data_df.dropna()\n","\n","reddit_data_df.printSchema()\n","reddit_data_df.select(col(\"subreddit_name_prefixed\")).show()"]},{"cell_type":"code","execution_count":5,"id":"97ef648a-d06e-4b18-b26c-66446439981e","metadata":{},"outputs":[],"source":["# for debug only, slice datasets\n","# # Assuming df is your original DataFrame\n","# games_df = reddit_data_df.filter(col(\"subreddit_name_prefixed\") == \"r/Games\").limit(100)\n","# diy_df = reddit_data_df.filter(col(\"subreddit_name_prefixed\") == \"r/DIY\").limit(100)\n","\n","# # If you need to combine these two DataFrames\n","# combined_df = games_df.union(diy_df)\n","\n","# # Show the result\n","# combined_df.groupBy(col(\"subreddit_name_prefixed\")).agg(count(\"*\")).show()\n"]},{"cell_type":"code","execution_count":6,"id":"979cbece-633c-4b42-a64e-2f3eaae192ab","metadata":{},"outputs":[],"source":["# twitter_data_df = spark.read.parquet(\"gs://msca-bdp-student-gcs/Group2_Final_Project/twitter_data/\",header=True, inferSchema=True)\n","# twitter_data_df = twitter_data_df.dropna()\n","# twitter_data_df.show()\n","# grouped_tw_by_usr = twitter_data_df.groupBy(col(\"user\")).agg(count(\"*\").alias(\"tweet_count\")).orderBy(col(\"tweet_count\"),ascending = False)\n","# tw_by_usr_dist = grouped_tw_by_usr.select(\"*\").groupBy(\"tweet_count\").agg(count(\"*\").alias(\"tweet_count_dist\")).orderBy(col(\"tweet_count\"))\n","# # grouped_tw_by_usr.show()\n","# tw_by_usr_dist.show()\n","# print(tw_by_usr_dist.count())"]},{"cell_type":"code","execution_count":7,"id":"bde6de00-0f3c-42d4-8259-15fb566565c8","metadata":{},"outputs":[],"source":["# Tokenize and stop word removal\n","def clean_text(text):\n","    # Deal with component words\n","    re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', text)\n","    # Convert to lowercase\n","    text = text.lower()\n","    # Remove Http / Https links in the text\n","    text = re.sub(r'http\\S+', '', text)\n","    text = re.sub(r'https\\S+', '', text)\n","    # Remove special characters and numbers\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","    # Handling repeated characters (more than 2)\n","    text = re.sub(r'(.)\\1+', r'\\1\\1', text)\n","    # Remove extra spaces\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text"]},{"cell_type":"code","execution_count":8,"id":"5b87fdb2-a1bd-42ec-94b2-dd11ac68d13e","metadata":{},"outputs":[],"source":["# For the reddit datasets, could use subreddits as labels and predict how likely this comment belongs to what subreddits\n","\n","# reddit_count_df = reddit_data_df.groupby(col(\"subreddit_name_prefixed\")).agg(count(\"*\").alias(\"reddit_count\")).orderBy(\"reddit_count\", ascending=False)\n","# reddit_count_df.show()\n"]},{"cell_type":"code","execution_count":9,"id":"8055980a-8efb-4067-b4e7-99147868f489","metadata":{},"outputs":[],"source":["# for debug only -- slice count datasets\n","# reddit_count_df = combined_df.groupby(col(\"subreddit_name_prefixed\")).agg(count(\"*\").alias(\"reddit_count\")).orderBy(\"reddit_count\", ascending=False)\n","# reddit_count_df.show()"]},{"cell_type":"code","execution_count":10,"id":"ff45b67c-6fcd-4f86-b08b-11b0500de368","metadata":{},"outputs":[],"source":["def encode_by_tags(df,labelcol):\n","    \n","    tags_lst = [row[labelcol] for row in df.select(col(labelcol)).distinct().collect()]\n","    df_res = df\n","    for tag in tags_lst:\n","        print(tag)\n","        df_res= df_res.withColumn(tag, when(col(labelcol) == tag, 1).otherwise(0))\n","\n","    return tags_lst, df_res"]},{"cell_type":"code","execution_count":11,"id":"1d1fe0c7-0e27-4bc0-b94a-382897834039","metadata":{},"outputs":[],"source":["\n","def clean_df(df, inputcol):\n","    clean_text_udf = udf(clean_text, StringType())\n","    df_cleaned = df.withColumn(inputcol, clean_text_udf(df[inputcol]))\n","    return df_cleaned\n","\n","def train_test_val_split(df, train_prob = 0.7, test_prob=0.2, val_prob= 0.1):\n","    train_df, test_df, validation_df = df.randomSplit([train_prob, test_prob, val_prob])\n","    return train_df, test_df, validation_df\n","    \n","\n","def preprocess_pipeline(labelcol, inputcol,finfeaturecol):\n","    # tokenize the comments into words\n","    tokenizer = Tokenizer(inputCol=inputcol, outputCol=\"token\")\n","    \n","    # remove stop words\n","    remover = StopWordsRemover(inputCol=\"token\", outputCol=\"filtered_token\")\n","    \n","    # vecotorize the words\n","    vectorizer = CountVectorizer(inputCol=\"filtered_token\", outputCol=\"features\")\n","    idf = IDF(inputCol=\"features\", outputCol=\"tfidf_features\")\n","    \n","    # assemble all features into 1 column\n","    assembler = VectorAssembler(inputCols=[\"features\",\"tfidf_features\"], outputCol=finfeaturecol)\n","\n","    # Create the preprocessing piplines for the tweets\n","    pipeline = Pipeline().setStages([\n","        tokenizer,\n","        remover,\n","        vectorizer,\n","        idf,\n","        assembler\n","    ])\n","    return pipeline\n","\n","def model_training_pipeline(featurecol, labelcol):\n","    svm = LinearSVC(labelCol=labelcol, featuresCol=featurecol)\n","    pipeline = Pipeline(stages=[svm])\n","    return svm,pipeline"]},{"cell_type":"code","execution_count":12,"id":"a5620ce3-3644-4e3c-8653-31a3e6e46369","metadata":{},"outputs":[],"source":["def find_best_svm_hyperparameters(df,featurecol,labelCol):\n","    \n","    svm,pipeline = model_training_pipeline(featurecol, labelcol)\n","    # Set up the parameter grid\n","    paramGrid = ParamGridBuilder() \\\n","        .addGrid(svm.maxIter, [10, 100]) \\\n","        .addGrid(svm.regParam, [0.01, 0.1, 1.0]) \\\n","        .build()\n","\n","    # Evaluator\n","    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=labelCol)\n","    df.select(col(featurecol)).show()\n","\n","    # Cross Validator\n","    crossval = CrossValidator(estimator=svm,\n","                              estimatorParamMaps=paramGrid,\n","                              evaluator=evaluator,\n","                              numFolds=3)\n","\n","    # Run cross-validation and choose the best model\n","    print(\"start fitting\")\n","    cvModel = crossval.fit(df)\n","    print(cvModel)\n","\n","    return cvModel.bestModel"]},{"cell_type":"code","execution_count":13,"id":"d3394287-9eef-422c-afe3-86673f9c83bb","metadata":{},"outputs":[],"source":["def list_all_files(bucket_name, folder_name):\n","    bucket = stoarge_client.bucket(bucket_name)\n","    file_lst = [('r/'+ blob.name.split(\"/\")[2].split(\"_\")[0]) for blob in bucket.list_blobs(prefix = folder_name)]\n","    file_lst = list(set(file_lst))\n","  \n","    return file_lst"]},{"cell_type":"code","execution_count":23,"id":"c248d747-92f2-4c4f-acd9-2b113d500240","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['r/lifehacks', 'r/Showerthoughts', 'r/Damnthatsinteresting', 'r/SkincareAddiction', 'r/Documentaries', 'r/IWantToLearn', 'r/socialskills', 'r/YouShouldKnow', 'r/tifu', 'r/Games', 'r/AskHistorians', 'r/DIY', 'r/scifi', 'r/gadgets', 'r/IAmA', 'r/space', 'r/personalfinance', 'r/science', 'r/', 'r/UpliftingNews', 'r/Fantasy', 'r/explainlikeimfive', 'r/femalefashionadvice', 'r/bodyweightfitness', 'r/podcasts', 'r/todayilearned', 'r/gardening']\n"]}],"source":["bucket_name = 'msca-bdp-student-gcs'\n","folder_name = 'Group2_Final_Project/modelr/'\n","trained_tags = list_all_files(bucket_name, folder_name)\n","\n","print(trained_tags)\n"]},{"cell_type":"code","execution_count":null,"id":"9e809b26-915f-42b6-8b04-64e028f0869e","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["r/YouShouldKnow\n","r/Fantasy\n","r/lifehacks\n","r/podcasts\n","r/UpliftingNews\n","r/AskHistorians\n","r/SkincareAddiction\n","r/IAmA\n","r/scifi\n","r/programming\n","r/Documentaries\n","r/todayilearned\n","r/gardening\n","r/IWantToLearn\n","r/science\n","r/explainlikeimfive\n","r/bodyweightfitness\n","r/bestof\n","r/Foodforthought\n","r/history\n","r/femalefashionadvice\n","r/Damnthatsinteresting\n","r/DIY\n","r/Showerthoughts\n","r/tifu\n","r/socialskills\n","r/Games\n","r/space\n","r/personalfinance\n","r/gadgets\n","r/LifeProTips\n","r/buildapc\n","r/boardgames\n","r/malefashionadvice\n","r/WritingPrompts\n","r/changemyview\n","r/philosophy\n","r/gaming\n","r/travel\n","r/technology\n","r/books\n","r/suggestmeabook\n","r/ifyoulikeblank\n","r/Fitness\n","r/GetMotivated\n","r/mildlyinteresting\n","r/EatCheapAndHealthy\n","r/sports\n","r/relationship_advice\n","r/askscience\n"]},{"name":"stderr","output_type":"stream","text":["23/11/26 03:53:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n","23/11/26 03:56:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n","23/11/26 03:56:09 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-------------+--------+\n","|r/LifeProTips|   count|\n","+-------------+--------+\n","|            1| 1835558|\n","|            0|64461618|\n","+-------------+--------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["23/11/26 03:57:46 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:00:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:00:21 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:02:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:02:52 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n","23/11/26 04:02:52 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n","23/11/26 04:02:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:03:13 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:03:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:03:29 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:03:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:03:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:04:01 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:04:15 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:04:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:04:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:04:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:05:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:05:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:05:22 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:05:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:05:46 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:05:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:06:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:06:07 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:06:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:06:31 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:06:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:06:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:07:07 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:07:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:07:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:07:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:07:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:07:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:07:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:08:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:08:15 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:08:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:08:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:08:43 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:08:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:08:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:09:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:09:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:09:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:09:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:09:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:09:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:10:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:10:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:10:25 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:10:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:10:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:10:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:11:07 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:11:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:11:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:11:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:11:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:11:43 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:11:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:12:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:12:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:12:18 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:12:32 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:12:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:12:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:12:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:13:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:13:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:13:21 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:13:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:13:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:13:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:13:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:14:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:14:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:14:21 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:14:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:14:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:14:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:14:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:15:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:15:18 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:15:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:15:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:15:50 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:15:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:16:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:16:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:16:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:16:26 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:16:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:16:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:16:57 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:17:01 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:17:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:17:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:17:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:17:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:17:57 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:18:01 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:18:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:18:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:18:32 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:18:35 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:18:50 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:18:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:19:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:19:11 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:19:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:19:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:19:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:19:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:20:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:20:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:20:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:20:22 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:20:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:20:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:20:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:20:57 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:21:11 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:21:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:21:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:21:31 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:21:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:21:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:22:01 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:22:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:22:15 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:22:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:22:31 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:22:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:22:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:22:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:23:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:23:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:23:21 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:23:26 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:23:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:23:41 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:23:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:23:57 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:24:13 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:24:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:24:25 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:24:29 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:24:42 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:24:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:25:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:25:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:25:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:25:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:25:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:25:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:25:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:25:57 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:26:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:26:13 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:26:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:26:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:26:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:26:43 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:26:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:26:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:27:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:27:15 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:27:29 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:27:32 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:27:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:27:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:28:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:28:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:28:18 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:28:21 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:28:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:28:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:28:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:28:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:29:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:29:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:29:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:29:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:29:43 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:29:46 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:30:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:30:07 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:30:21 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:30:25 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:30:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:30:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:30:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:31:01 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:31:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:31:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:31:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:31:42 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:31:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:31:57 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:32:13 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:32:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:32:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:32:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:32:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:32:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:33:01 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:33:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:33:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:33:18 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:33:32 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:33:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:33:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:33:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:34:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:34:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:34:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:34:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:34:42 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:34:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:34:57 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:35:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:35:11 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:35:15 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:35:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:35:31 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:35:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:35:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:36:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:36:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:36:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:36:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:36:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:36:43 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:36:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:37:01 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:37:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:37:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:37:32 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:37:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:37:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:37:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:38:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:38:07 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:38:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:38:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:38:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:38:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:38:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:38:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:39:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:39:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:39:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:39:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:39:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:39:42 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:39:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:39:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:40:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:40:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:40:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:40:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:40:41 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:40:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:40:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:41:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:41:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:41:18 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:41:32 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:41:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:41:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:41:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:42:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:42:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:42:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:42:26 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:42:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:42:43 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:42:57 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:43:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:43:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:43:13 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:43:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:43:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:43:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:43:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:44:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:44:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:44:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:44:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:44:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:44:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:44:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:44:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:45:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:45:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:45:26 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:45:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:45:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:45:43 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:45:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:45:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:46:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:46:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:46:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:46:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:46:42 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:46:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:46:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:47:01 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:47:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:47:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:47:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:47:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:47:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:47:50 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:48:01 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:48:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:48:18 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:48:22 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:48:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:48:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:48:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:48:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:49:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:49:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:49:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:49:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:49:46 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:49:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:50:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:50:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:50:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:50:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:50:35 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:50:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:50:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:50:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:51:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:51:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:51:22 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:51:25 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:51:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:51:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:51:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:51:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:52:07 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:52:11 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:52:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:52:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:52:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:52:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:52:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:53:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:53:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:53:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:53:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:53:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:53:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:53:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:54:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:54:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:54:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:54:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:54:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:54:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:54:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:55:01 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:55:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:55:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:55:29 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:55:32 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:55:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:55:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:56:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:56:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:56:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:56:26 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:56:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:56:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:56:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:56:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:57:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:57:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:57:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:57:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:57:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:57:42 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:57:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:57:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:58:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:58:13 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:58:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:58:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:58:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:58:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:58:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:58:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:59:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:59:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:59:25 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:59:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:59:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:59:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 04:59:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:00:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:00:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:00:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:00:43 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:00:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:01:01 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:01:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:01:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:01:21 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:01:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:01:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:01:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:01:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:02:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:02:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:02:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:02:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:13:26 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 41.7 MiB\n","23/11/26 05:15:03 WARN org.apache.spark.scheduler.TaskSetManager: Stage 500 contains a task of very large size (4184 KiB). The maximum recommended task size is 1000 KiB.\n","23/11/26 05:16:57 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n","23/11/26 05:18:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+----------+--------+\n","|r/buildapc|   count|\n","+----------+--------+\n","|         1| 2126162|\n","|         0|64170174|\n","+----------+--------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["23/11/26 05:20:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:22:26 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:22:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:24:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:24:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:24:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:25:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:25:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:25:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:25:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:25:41 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:25:50 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:25:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:26:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:26:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:26:22 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:26:26 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:26:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:26:41 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:26:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:26:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:27:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:27:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:27:18 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:27:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:27:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:27:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:27:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 05:27:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:03:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:03:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:03:41 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:03:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:03:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:04:11 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:04:15 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:04:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:04:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:04:43 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:04:46 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:05:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:05:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:05:21 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:05:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:05:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:05:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:05:57 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:06:01 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:06:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:06:15 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:06:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:06:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:06:42 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:06:46 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:06:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:07:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:07:15 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:07:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:07:32 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:07:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:07:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:07:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:08:01 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:08:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:08:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:08:21 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:08:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:08:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:08:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:08:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:09:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:09:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:09:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:09:21 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:09:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:09:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:09:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:09:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:10:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:10:07 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:10:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:10:22 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:10:31 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:10:35 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:10:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:10:50 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:11:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:11:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:11:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:11:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:11:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:11:31 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:11:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:11:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:12:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:12:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:12:18 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:12:21 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:12:31 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:12:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:12:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:12:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:13:01 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:13:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:13:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:13:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:13:31 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:13:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:13:50 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:13:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:14:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:14:11 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:14:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:14:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:14:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:14:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:14:57 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:15:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:15:11 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:15:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:15:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:15:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:15:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:15:43 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:15:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:15:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:16:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:16:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:16:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:16:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:16:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:16:41 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:16:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:16:57 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:17:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:17:15 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:17:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:17:31 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:17:42 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:17:46 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:17:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:17:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:18:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:18:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:18:25 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:18:29 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:18:42 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:18:46 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:18:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:18:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:19:13 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:19:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:19:26 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:19:29 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:19:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:19:42 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:19:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:19:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:20:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:20:15 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:20:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:20:31 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:20:43 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:20:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:20:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:21:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:21:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:21:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:21:29 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:21:32 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:21:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:21:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:22:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:22:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:22:18 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:22:22 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:22:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:22:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:22:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:22:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:23:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:23:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:23:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:23:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:23:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:23:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:23:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:23:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:24:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:24:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:24:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:24:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:24:32 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:24:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:24:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:24:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:25:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:25:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:25:18 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:25:22 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:25:31 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:25:35 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:25:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:25:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:26:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:26:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:26:18 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:26:22 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:26:35 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:26:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:26:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:26:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:27:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:27:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:27:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:27:22 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:27:35 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:27:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:27:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:27:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:28:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:28:11 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:28:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:28:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:28:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:28:43 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:28:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:28:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:29:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:29:13 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:29:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:29:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:29:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:29:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:29:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:29:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:30:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:30:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:30:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:30:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:30:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:30:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:30:43 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:30:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:30:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:31:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:31:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:31:13 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:31:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:31:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:31:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:31:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:31:50 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:31:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:32:07 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:32:11 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:32:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:32:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:32:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:32:42 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:32:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:32:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:33:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:33:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:33:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:33:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:33:35 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:33:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:33:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:33:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:34:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:34:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:34:21 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:34:25 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:34:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:34:41 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:34:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:34:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:35:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:35:13 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:35:25 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:35:29 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:35:43 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:35:46 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:35:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:35:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:36:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:36:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:36:22 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:36:26 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:36:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:36:43 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:36:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:36:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:37:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:37:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:37:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:37:32 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:37:43 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:37:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:37:57 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:38:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:38:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:38:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:38:25 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:38:29 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:38:41 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:38:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 41.7 MiB\n","23/11/26 06:40:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 41.7 MiB\n","23/11/26 06:42:21 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 41.7 MiB\n","23/11/26 06:44:02 WARN org.apache.spark.scheduler.TaskSetManager: Stage 1104 contains a task of very large size (4184 KiB). The maximum recommended task size is 1000 KiB.\n","23/11/26 06:46:01 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n","23/11/26 06:48:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+------------+--------+\n","|r/boardgames|   count|\n","+------------+--------+\n","|           1|  720138|\n","|           0|65565597|\n","+------------+--------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["23/11/26 06:49:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:52:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:52:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:54:41 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:54:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:55:13 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:55:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:55:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:55:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:55:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:55:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:56:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:56:13 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:56:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:56:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:56:42 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:56:46 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:57:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:57:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:57:15 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:57:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:57:29 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:57:32 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:57:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:57:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:57:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:58:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:58:13 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:58:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:58:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:58:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:58:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:58:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:58:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:59:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:59:13 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:59:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:59:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:59:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:59:42 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:59:46 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 06:59:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:00:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:00:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:00:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:00:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:00:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:00:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:00:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:01:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:01:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:01:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:01:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:01:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:01:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:01:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:01:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:02:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:02:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:02:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:02:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:02:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:02:42 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:02:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:02:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:03:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:03:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:03:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:03:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:03:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:03:42 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:03:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:03:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:04:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:04:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:04:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:04:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:04:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:04:42 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:04:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:04:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:05:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:05:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:05:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:05:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:05:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:05:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:05:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:05:50 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:06:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:06:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:06:18 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:06:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:06:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:06:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:06:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:06:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:07:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:07:13 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:07:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:07:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:07:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:07:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:07:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:07:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:08:11 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:08:15 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:08:25 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:08:29 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:08:43 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:08:46 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:09:01 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:09:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:09:15 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:09:18 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:09:31 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:09:35 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:09:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:09:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:10:01 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:10:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:10:18 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:10:22 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:10:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:10:42 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:10:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:10:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:11:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:11:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:11:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:11:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:11:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:11:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:11:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:11:57 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:12:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:12:13 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:12:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:12:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:12:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:12:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:12:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:13:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:13:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:13:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:13:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:13:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:13:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:13:50 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:13:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:14:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:14:15 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:14:18 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:14:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:14:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:14:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:14:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:15:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:15:07 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:15:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:15:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:15:31 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:15:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:15:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:15:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:15:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:16:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:16:13 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:16:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:16:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:16:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:16:41 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:16:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:16:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:16:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:17:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:17:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:17:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:17:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:17:41 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:17:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:17:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:18:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:18:11 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:18:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:18:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:18:31 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:18:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:18:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:19:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:19:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:19:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:19:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:19:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:19:31 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:19:42 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:19:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:20:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:20:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:20:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:20:21 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:20:32 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:20:35 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:20:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:20:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:21:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:21:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:21:21 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:21:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:21:35 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:21:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:21:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:21:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:22:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:22:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:22:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:22:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:22:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:22:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:22:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:22:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:23:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:23:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:23:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:23:22 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:23:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:23:41 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:23:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:23:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:24:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:24:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:24:22 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:24:26 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:24:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:24:41 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:24:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:24:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:25:07 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1700913432325_0006_01_000064 on host: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal. Exit status: 143. Diagnostics: [2023-11-26 07:25:07.212]Container killed on request. Exit code is 143\n","[2023-11-26 07:25:07.214]Container exited with a non-zero exit code 143. \n","[2023-11-26 07:25:07.216]Killed by external signal\n",".\n","23/11/26 07:25:07 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1700913432325_0006_01_000054 on host: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal. Exit status: 143. Diagnostics: [2023-11-26 07:25:07.212]Container killed on request. Exit code is 143\n","[2023-11-26 07:25:07.212]Container exited with a non-zero exit code 143. \n","[2023-11-26 07:25:07.213]Killed by external signal\n",".\n","23/11/26 07:25:07 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 63 on hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal: Container from a bad node: container_1700913432325_0006_01_000064 on host: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal. Exit status: 143. Diagnostics: [2023-11-26 07:25:07.212]Container killed on request. Exit code is 143\n","[2023-11-26 07:25:07.214]Container exited with a non-zero exit code 143. \n","[2023-11-26 07:25:07.216]Killed by external signal\n",".\n","23/11/26 07:25:07 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 63 for reason Container from a bad node: container_1700913432325_0006_01_000064 on host: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal. Exit status: 143. Diagnostics: [2023-11-26 07:25:07.212]Container killed on request. Exit code is 143\n","[2023-11-26 07:25:07.214]Container exited with a non-zero exit code 143. \n","[2023-11-26 07:25:07.216]Killed by external signal\n",".\n","23/11/26 07:25:07 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 53 for reason Container from a bad node: container_1700913432325_0006_01_000054 on host: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal. Exit status: 143. Diagnostics: [2023-11-26 07:25:07.212]Container killed on request. Exit code is 143\n","[2023-11-26 07:25:07.212]Container exited with a non-zero exit code 143. \n","[2023-11-26 07:25:07.213]Killed by external signal\n",".\n","23/11/26 07:25:07 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 53 on hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal: Container from a bad node: container_1700913432325_0006_01_000054 on host: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal. Exit status: 143. Diagnostics: [2023-11-26 07:25:07.212]Container killed on request. Exit code is 143\n","[2023-11-26 07:25:07.212]Container exited with a non-zero exit code 143. \n","[2023-11-26 07:25:07.213]Killed by external signal\n",".\n","23/11/26 07:25:07 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 859.0 in stage 1354.0 (TID 659444) (hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal executor 53): ExecutorLostFailure (executor 53 exited caused by one of the running tasks) Reason: Container from a bad node: container_1700913432325_0006_01_000054 on host: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal. Exit status: 143. Diagnostics: [2023-11-26 07:25:07.212]Container killed on request. Exit code is 143\n","[2023-11-26 07:25:07.212]Container exited with a non-zero exit code 143. \n","[2023-11-26 07:25:07.213]Killed by external signal\n",".\n","23/11/26 07:25:07 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 767.0 in stage 1354.0 (TID 659411) (hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal executor 53): ExecutorLostFailure (executor 53 exited caused by one of the running tasks) Reason: Container from a bad node: container_1700913432325_0006_01_000054 on host: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal. Exit status: 143. Diagnostics: [2023-11-26 07:25:07.212]Container killed on request. Exit code is 143\n","[2023-11-26 07:25:07.212]Container exited with a non-zero exit code 143. \n","[2023-11-26 07:25:07.213]Killed by external signal\n",".\n","23/11/26 07:25:07 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 483.0 in stage 1354.0 (TID 659389) (hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal executor 53): ExecutorLostFailure (executor 53 exited caused by one of the running tasks) Reason: Container from a bad node: container_1700913432325_0006_01_000054 on host: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal. Exit status: 143. Diagnostics: [2023-11-26 07:25:07.212]Container killed on request. Exit code is 143\n","[2023-11-26 07:25:07.212]Container exited with a non-zero exit code 143. \n","[2023-11-26 07:25:07.213]Killed by external signal\n",".\n","23/11/26 07:25:07 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 904.0 in stage 1354.0 (TID 659422) (hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal executor 53): ExecutorLostFailure (executor 53 exited caused by one of the running tasks) Reason: Container from a bad node: container_1700913432325_0006_01_000054 on host: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal. Exit status: 143. Diagnostics: [2023-11-26 07:25:07.212]Container killed on request. Exit code is 143\n","[2023-11-26 07:25:07.212]Container exited with a non-zero exit code 143. \n","[2023-11-26 07:25:07.213]Killed by external signal\n",".\n","23/11/26 07:25:07 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 782.0 in stage 1354.0 (TID 659433) (hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal executor 53): ExecutorLostFailure (executor 53 exited caused by one of the running tasks) Reason: Container from a bad node: container_1700913432325_0006_01_000054 on host: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal. Exit status: 143. Diagnostics: [2023-11-26 07:25:07.212]Container killed on request. Exit code is 143\n","[2023-11-26 07:25:07.212]Container exited with a non-zero exit code 143. \n","[2023-11-26 07:25:07.213]Killed by external signal\n",".\n","23/11/26 07:25:07 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 616.0 in stage 1354.0 (TID 659400) (hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal executor 53): ExecutorLostFailure (executor 53 exited caused by one of the running tasks) Reason: Container from a bad node: container_1700913432325_0006_01_000054 on host: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal. Exit status: 143. Diagnostics: [2023-11-26 07:25:07.212]Container killed on request. Exit code is 143\n","[2023-11-26 07:25:07.212]Container exited with a non-zero exit code 143. \n","[2023-11-26 07:25:07.213]Killed by external signal\n",".\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_401 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_168 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_444 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_110 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_23 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_398 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_282 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_197 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_474 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_580 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_568 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_400 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_587 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_518 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_325 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_352 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_621 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_52 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_556 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_528 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_639 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_278 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_890 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_139 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_343 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_81 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_226 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_515 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_630 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_437 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_684 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_346 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_446 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_763 !\n","23/11/26 07:25:07 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_2442_704 !\n","23/11/26 07:25:11 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:27:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 10.0 in stage 1355.0 (TID 659469) (hub-msca-bdp-dphub-students-test-qinglin-sw-0nkf.c.msca-bdp-student-ap.internal executor 64): FetchFailed(BlockManagerId(53, hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal, 7337, None), shuffleId=664, mapIndex=400, mapId=658788, reduceId=10, message=\n","org.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n","\tat org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)\n","\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:112)\n","\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n","\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n","\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","Caused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal\n","\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n","\tat java.net.InetAddress.getAllByName0(InetAddress.java:1291)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1144)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1065)\n","\tat java.net.InetAddress.getByName(InetAddress.java:1015)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n","\tat java.security.AccessController.doPrivileged(Native Method)\n","\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n","\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n","\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n","\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n","\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n","\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n","\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n","\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n","\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n","\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\t... 2 more\n","\n",")\n","23/11/26 07:27:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 28.0 in stage 1355.0 (TID 659476) (hub-msca-bdp-dphub-students-test-qinglin-w-3.c.msca-bdp-student-ap.internal executor 17): FetchFailed(BlockManagerId(53, hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal, 7337, None), shuffleId=664, mapIndex=568, mapId=659055, reduceId=28, message=\n","org.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n","\tat org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)\n","\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:112)\n","\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n","\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n","\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","Caused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal\n","\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n","\tat java.net.InetAddress.getAllByName0(InetAddress.java:1291)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1144)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1065)\n","\tat java.net.InetAddress.getByName(InetAddress.java:1015)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n","\tat java.security.AccessController.doPrivileged(Native Method)\n","\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n","\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n","\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n","\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n","\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n","\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n","\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n","\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n","\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n","\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\t... 2 more\n","\n",")\n","23/11/26 07:27:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 3.0 in stage 1355.0 (TID 659471) (hub-msca-bdp-dphub-students-test-qinglin-w-6.c.msca-bdp-student-ap.internal executor 60): FetchFailed(BlockManagerId(63, hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal, 7337, None), shuffleId=664, mapIndex=753, mapId=659341, reduceId=3, message=\n","org.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n","\tat org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)\n","\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:112)\n","\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n","\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n","\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","Caused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal\n","\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n","\tat java.net.InetAddress.getAllByName0(InetAddress.java:1291)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1144)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1065)\n","\tat java.net.InetAddress.getByName(InetAddress.java:1015)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n","\tat java.security.AccessController.doPrivileged(Native Method)\n","\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n","\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n","\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n","\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n","\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n","\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n","\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n","\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n","\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n","\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\t... 2 more\n","\n",")\n","23/11/26 07:27:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 19.0 in stage 1355.0 (TID 659477) (hub-msca-bdp-dphub-students-test-qinglin-sw-0zmt.c.msca-bdp-student-ap.internal executor 29): FetchFailed(BlockManagerId(53, hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal, 7337, None), shuffleId=664, mapIndex=139, mapId=658625, reduceId=19, message=\n","org.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n","\tat org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)\n","\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:112)\n","\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n","\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n","\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","Caused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal\n","\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n","\tat java.net.InetAddress.getAllByName0(InetAddress.java:1291)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1144)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1065)\n","\tat java.net.InetAddress.getByName(InetAddress.java:1015)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n","\tat java.security.AccessController.doPrivileged(Native Method)\n","\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n","\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n","\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n","\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n","\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n","\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n","\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n","\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n","\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n","\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\t... 2 more\n","\n",")\n","23/11/26 07:27:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 23.0 in stage 1355.0 (TID 659478) (hub-msca-bdp-dphub-students-test-qinglin-sw-0zmt.c.msca-bdp-student-ap.internal executor 31): FetchFailed(BlockManagerId(53, hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal, 7337, None), shuffleId=664, mapIndex=23, mapId=658509, reduceId=23, message=\n","org.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337 failed in the last 4750 ms, fail this connection directly\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n","\tat org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)\n","\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:112)\n","\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337 failed in the last 4750 ms, fail this connection directly\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n","\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n","\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","\n",")\n","23/11/26 07:27:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 18.0 in stage 1355.0 (TID 659475) (hub-msca-bdp-dphub-students-test-qinglin-sw-0zmt.c.msca-bdp-student-ap.internal executor 31): FetchFailed(BlockManagerId(53, hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal, 7337, None), shuffleId=664, mapIndex=168, mapId=658654, reduceId=18, message=\n","org.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n","\tat org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)\n","\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:112)\n","\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n","\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n","\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","Caused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal\n","\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n","\tat java.net.InetAddress.getAllByName0(InetAddress.java:1291)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1144)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1065)\n","\tat java.net.InetAddress.getByName(InetAddress.java:1015)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n","\tat java.security.AccessController.doPrivileged(Native Method)\n","\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n","\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n","\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n","\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n","\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n","\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n","\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n","\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n","\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n","\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\t... 2 more\n","\n",")\n","23/11/26 07:27:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 14.0 in stage 1355.0 (TID 659466) (hub-msca-bdp-dphub-students-test-qinglin-sw-890g.c.msca-bdp-student-ap.internal executor 30): FetchFailed(BlockManagerId(63, hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal, 7337, None), shuffleId=664, mapIndex=704, mapId=659079, reduceId=14, message=\n","org.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n","\tat org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)\n","\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:112)\n","\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n","\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n","\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","Caused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal\n","\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n","\tat java.net.InetAddress.getAllByName0(InetAddress.java:1291)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1144)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1065)\n","\tat java.net.InetAddress.getByName(InetAddress.java:1015)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n","\tat java.security.AccessController.doPrivileged(Native Method)\n","\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n","\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n","\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n","\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n","\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n","\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n","\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n","\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n","\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n","\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\t... 2 more\n","\n",")\n","23/11/26 07:27:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 5.0 in stage 1355.0 (TID 659468) (hub-msca-bdp-dphub-students-test-qinglin-w-2.c.msca-bdp-student-ap.internal executor 25): FetchFailed(BlockManagerId(53, hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal, 7337, None), shuffleId=664, mapIndex=515, mapId=658977, reduceId=5, message=\n","org.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n","\tat org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)\n","\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:112)\n","\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n","\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n","\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","Caused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal\n","\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n","\tat java.net.InetAddress.getAllByName0(InetAddress.java:1291)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1144)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1065)\n","\tat java.net.InetAddress.getByName(InetAddress.java:1015)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n","\tat java.security.AccessController.doPrivileged(Native Method)\n","\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n","\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n","\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n","\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n","\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n","\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n","\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n","\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n","\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n","\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\t... 2 more\n","\n",")\n","23/11/26 07:27:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 16.0 in stage 1355.0 (TID 659483) (hub-msca-bdp-dphub-students-test-qinglin-w-6.c.msca-bdp-student-ap.internal executor 5): FetchFailed(BlockManagerId(53, hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal, 7337, None), shuffleId=664, mapIndex=226, mapId=658712, reduceId=16, message=\n","org.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337 failed in the last 4750 ms, fail this connection directly\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n","\tat org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)\n","\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:112)\n","\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337 failed in the last 4750 ms, fail this connection directly\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n","\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n","\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","\n",")\n","23/11/26 07:27:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 1355.0 (TID 659462) (hub-msca-bdp-dphub-students-test-qinglin-w-6.c.msca-bdp-student-ap.internal executor 5): FetchFailed(BlockManagerId(63, hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal, 7337, None), shuffleId=664, mapIndex=630, mapId=659072, reduceId=0, message=\n","org.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n","\tat org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)\n","\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:112)\n","\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n","\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n","\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","Caused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal\n","\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n","\tat java.net.InetAddress.getAllByName0(InetAddress.java:1291)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1144)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1065)\n","\tat java.net.InetAddress.getByName(InetAddress.java:1015)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n","\tat java.security.AccessController.doPrivileged(Native Method)\n","\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n","\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n","\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n","\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n","\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n","\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n","\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n","\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n","\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n","\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\t... 2 more\n","\n",")\n","23/11/26 07:27:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 20.0 in stage 1355.0 (TID 659465) (hub-msca-bdp-dphub-students-test-qinglin-sw-31l9.c.msca-bdp-student-ap.internal executor 27): FetchFailed(BlockManagerId(63, hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal, 7337, None), shuffleId=664, mapIndex=800, mapId=659344, reduceId=20, message=\n","org.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n","\tat org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)\n","\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:112)\n","\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n","\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n","\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","Caused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal\n","\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n","\tat java.net.InetAddress.getAllByName0(InetAddress.java:1291)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1144)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1065)\n","\tat java.net.InetAddress.getByName(InetAddress.java:1015)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n","\tat java.security.AccessController.doPrivileged(Native Method)\n","\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n","\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n","\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n","\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n","\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n","\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n","\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n","\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n","\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n","\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\t... 2 more\n","\n",")\n","23/11/26 07:27:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 11.0 in stage 1355.0 (TID 659472) (hub-msca-bdp-dphub-students-test-qinglin-w-0.c.msca-bdp-student-ap.internal executor 12): FetchFailed(BlockManagerId(53, hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal, 7337, None), shuffleId=664, mapIndex=401, mapId=658960, reduceId=11, message=\n","org.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n","\tat org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)\n","\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:112)\n","\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n","\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n","\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","Caused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal\n","\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n","\tat java.net.InetAddress.getAllByName0(InetAddress.java:1291)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1144)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1065)\n","\tat java.net.InetAddress.getByName(InetAddress.java:1015)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n","\tat java.security.AccessController.doPrivileged(Native Method)\n","\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n","\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n","\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n","\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n","\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n","\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n","\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n","\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n","\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n","\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\t... 2 more\n","\n",")\n","23/11/26 07:27:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 26.0 in stage 1355.0 (TID 659474) (hub-msca-bdp-dphub-students-test-qinglin-w-0.c.msca-bdp-student-ap.internal executor 8): FetchFailed(BlockManagerId(53, hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal, 7337, None), shuffleId=664, mapIndex=446, mapId=658971, reduceId=26, message=\n","org.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n","\tat org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)\n","\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:112)\n","\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n","\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n","\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","Caused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal\n","\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n","\tat java.net.InetAddress.getAllByName0(InetAddress.java:1291)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1144)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1065)\n","\tat java.net.InetAddress.getByName(InetAddress.java:1015)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n","\tat java.security.AccessController.doPrivileged(Native Method)\n","\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n","\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n","\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n","\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n","\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n","\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n","\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n","\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n","\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n","\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\t... 2 more\n","\n",")\n","23/11/26 07:27:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 7.0 in stage 1355.0 (TID 659460) (hub-msca-bdp-dphub-students-test-qinglin-w-1.c.msca-bdp-student-ap.internal executor 13): FetchFailed(BlockManagerId(63, hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal, 7337, None), shuffleId=664, mapIndex=847, mapId=659347, reduceId=7, message=\n","org.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337 failed in the last 4750 ms, fail this connection directly\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n","\tat org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)\n","\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:112)\n","\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337 failed in the last 4750 ms, fail this connection directly\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n","\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n","\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","\n",")\n","23/11/26 07:27:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 8.0 in stage 1355.0 (TID 659481) (hub-msca-bdp-dphub-students-test-qinglin-w-1.c.msca-bdp-student-ap.internal executor 13): FetchFailed(BlockManagerId(53, hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal, 7337, None), shuffleId=664, mapIndex=278, mapId=658763, reduceId=8, message=\n","org.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n","\tat org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)\n","\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:112)\n","\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n","\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n","\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","Caused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal\n","\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n","\tat java.net.InetAddress.getAllByName0(InetAddress.java:1291)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1144)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1065)\n","\tat java.net.InetAddress.getByName(InetAddress.java:1015)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n","\tat java.security.AccessController.doPrivileged(Native Method)\n","\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n","\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n","\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n","\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n","\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n","\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n","\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n","\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n","\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n","\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\t... 2 more\n","\n",")\n","23/11/26 07:27:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 9.0 in stage 1355.0 (TID 659458) (hub-msca-bdp-dphub-students-test-qinglin-w-5.c.msca-bdp-student-ap.internal executor 1): FetchFailed(BlockManagerId(63, hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal, 7337, None), shuffleId=664, mapIndex=639, mapId=659075, reduceId=9, message=\n","org.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n","\tat org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)\n","\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:112)\n","\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n","\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n","\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","Caused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal\n","\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n","\tat java.net.InetAddress.getAllByName0(InetAddress.java:1291)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1144)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1065)\n","\tat java.net.InetAddress.getByName(InetAddress.java:1015)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n","\tat java.security.AccessController.doPrivileged(Native Method)\n","\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n","\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n","\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n","\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n","\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n","\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n","\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n","\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n","\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n","\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\t... 2 more\n","\n",")\n","23/11/26 07:27:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 21.0 in stage 1355.0 (TID 659463) (hub-msca-bdp-dphub-students-test-qinglin-w-5.c.msca-bdp-student-ap.internal executor 16): FetchFailed(BlockManagerId(63, hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal, 7337, None), shuffleId=664, mapIndex=621, mapId=659069, reduceId=21, message=\n","org.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337 failed in the last 4750 ms, fail this connection directly\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n","\tat org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)\n","\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:112)\n","\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337 failed in the last 4750 ms, fail this connection directly\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n","\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n","\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","\n",")\n","23/11/26 07:27:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 17.0 in stage 1355.0 (TID 659484) (hub-msca-bdp-dphub-students-test-qinglin-w-5.c.msca-bdp-student-ap.internal executor 16): FetchFailed(BlockManagerId(53, hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal, 7337, None), shuffleId=664, mapIndex=197, mapId=658683, reduceId=17, message=\n","org.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n","\tat org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)\n","\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:112)\n","\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n","\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n","\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","Caused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal\n","\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n","\tat java.net.InetAddress.getAllByName0(InetAddress.java:1291)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1144)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1065)\n","\tat java.net.InetAddress.getByName(InetAddress.java:1015)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n","\tat java.security.AccessController.doPrivileged(Native Method)\n","\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n","\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n","\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n","\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n","\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n","\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n","\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n","\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n","\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n","\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\t... 2 more\n","\n",")\n","23/11/26 07:27:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 25.0 in stage 1355.0 (TID 659470) (hub-msca-bdp-dphub-students-test-qinglin-sw-31l9.c.msca-bdp-student-ap.internal executor 21): FetchFailed(BlockManagerId(53, hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal, 7337, None), shuffleId=664, mapIndex=325, mapId=658765, reduceId=25, message=\n","org.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n","\tat org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)\n","\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:112)\n","\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n","\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n","\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","Caused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal\n","\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n","\tat java.net.InetAddress.getAllByName0(InetAddress.java:1291)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1144)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1065)\n","\tat java.net.InetAddress.getByName(InetAddress.java:1015)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n","\tat java.security.AccessController.doPrivileged(Native Method)\n","\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n","\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n","\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n","\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n","\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n","\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n","\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n","\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n","\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n","\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\t... 2 more\n","\n",")\n","23/11/26 07:27:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 12.0 in stage 1355.0 (TID 659482) (hub-msca-bdp-dphub-students-test-qinglin-sw-hjjp.c.msca-bdp-student-ap.internal executor 3): FetchFailed(BlockManagerId(53, hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal, 7337, None), shuffleId=664, mapIndex=282, mapId=658764, reduceId=12, message=\n","org.apache.spark.shuffle.FetchFailedException: Connecting to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337 failed in the last 4750 ms, fail this connection directly\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n","\tat org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)\n","\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:112)\n","\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: java.io.IOException: Connecting to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337 failed in the last 4750 ms, fail this connection directly\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n","\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n","\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","\n",")\n","23/11/26 07:27:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 13.0 in stage 1355.0 (TID 659461) (hub-msca-bdp-dphub-students-test-qinglin-sw-hjjp.c.msca-bdp-student-ap.internal executor 3): FetchFailed(BlockManagerId(53, hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal, 7337, None), shuffleId=664, mapIndex=343, mapId=658766, reduceId=13, message=\n","org.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n","\tat org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)\n","\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:112)\n","\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n","\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n","\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","Caused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal\n","\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n","\tat java.net.InetAddress.getAllByName0(InetAddress.java:1291)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1144)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1065)\n","\tat java.net.InetAddress.getByName(InetAddress.java:1015)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n","\tat java.security.AccessController.doPrivileged(Native Method)\n","\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n","\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n","\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n","\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n","\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n","\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n","\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n","\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n","\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n","\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\t... 2 more\n","\n",")\n","23/11/26 07:27:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:27:30 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 24.0 in stage 1355.0 (TID 659487) (hub-msca-bdp-dphub-students-test-qinglin-w-2.c.msca-bdp-student-ap.internal executor 19): FetchFailed(BlockManagerId(53, hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal, 7337, None), shuffleId=664, mapIndex=444, mapId=658969, reduceId=24, message=\n","org.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n","\tat org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)\n","\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:112)\n","\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal:7337\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n","\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n","\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n","\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","Caused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-test-qinglin-sw-xrsq.c.msca-bdp-student-ap.internal\n","\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n","\tat java.net.InetAddress.getAllByName0(InetAddress.java:1291)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1144)\n","\tat java.net.InetAddress.getAllByName(InetAddress.java:1065)\n","\tat java.net.InetAddress.getByName(InetAddress.java:1015)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n","\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n","\tat java.security.AccessController.doPrivileged(Native Method)\n","\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n","\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n","\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n","\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n","\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n","\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n","\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n","\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n","\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n","\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n","\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n","\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n","\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\t... 2 more\n","\n",")\n","23/11/26 07:29:01 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:29:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:29:21 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:29:25 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:29:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 1355.0 (TID 659464) (hub-msca-bdp-dphub-students-test-qinglin-sw-89h8.c.msca-bdp-student-ap.internal executor 14): TaskKilled (Stage finished)\n","23/11/26 07:29:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 22.0 in stage 1355.0 (TID 659485) (hub-msca-bdp-dphub-students-test-qinglin-sw-89h8.c.msca-bdp-student-ap.internal executor 14): TaskKilled (Stage finished)\n","23/11/26 07:29:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:29:41 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:29:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:29:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:30:11 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:30:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:30:25 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:30:29 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:30:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:30:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:31:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:31:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:31:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:31:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:31:31 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:31:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:31:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:31:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:32:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:32:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:32:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:32:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:32:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:32:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:32:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:32:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:33:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:33:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:33:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:33:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:33:35 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:33:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:33:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:33:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:34:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:34:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:34:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:34:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:34:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:34:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:34:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:34:50 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:35:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:35:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:35:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:35:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:35:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:35:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:35:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:35:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:36:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:36:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:36:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:36:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:36:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:36:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:37:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:37:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:37:22 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:37:26 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:37:41 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:37:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:37:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:37:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:38:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:38:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:38:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:38:31 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:38:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:38:57 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:39:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:39:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:39:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:39:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:39:50 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:39:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:40:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:40:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:40:21 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:40:25 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:40:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:40:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:41:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:41:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:41:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:41:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:41:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:41:42 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:41:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:41:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:42:07 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:42:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:42:22 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:42:26 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:42:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:42:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:42:57 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:43:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:43:11 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:43:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:43:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:43:29 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:43:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:43:42 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:43:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:43:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:44:07 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:44:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:44:29 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:44:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:44:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:44:50 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:45:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:45:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:45:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:45:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:45:32 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:45:35 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:45:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:45:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:46:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:46:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:46:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:46:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:46:31 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:46:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:46:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:46:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:47:07 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:47:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:47:21 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:47:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:47:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:47:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:47:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:47:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:48:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:48:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:48:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:48:32 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:48:43 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:48:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:49:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:49:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:49:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:49:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:49:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:49:42 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:49:57 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:50:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:50:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:50:18 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:50:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:50:32 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:50:46 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:50:50 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:51:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:51:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:51:22 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:51:26 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:51:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:51:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:51:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:51:57 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:52:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:52:11 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:52:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:52:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:52:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:52:41 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:52:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:52:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:53:07 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:53:11 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:53:21 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:53:25 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:53:35 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:53:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:53:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:53:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:54:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:54:07 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:54:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:54:22 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:54:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:54:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:54:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:54:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:55:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:55:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:55:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:55:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:55:35 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:55:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:55:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:55:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:56:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:56:07 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:56:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:56:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:56:35 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:56:41 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:56:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:56:57 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:57:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:57:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:57:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:57:32 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:57:46 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:57:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:58:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:58:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:58:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:58:21 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:58:31 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:58:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:58:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:58:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:59:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:59:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:59:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:59:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:59:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:59:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:59:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 07:59:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:00:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:00:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:00:22 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:00:25 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:00:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:00:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:00:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:01:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:01:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:01:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:01:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:01:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:01:41 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:01:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:01:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:01:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:02:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:02:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:02:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:02:26 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:02:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:02:41 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:02:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:02:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:03:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:03:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:03:26 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:03:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:03:43 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:03:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:03:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:04:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:04:13 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:04:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:04:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:04:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:04:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:04:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:04:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:05:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:05:15 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:05:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:05:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:05:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:05:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:05:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:06:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:06:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:06:26 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:06:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:06:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:06:43 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:06:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:06:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:07:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:07:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:07:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:07:32 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:07:43 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:07:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:08:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:08:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:08:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:08:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:08:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:08:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:08:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:08:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:09:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:09:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:09:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 08:09:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:14:35 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:14:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:14:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:14:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:15:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:15:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:15:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:15:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:15:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:15:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:15:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:16:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:16:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:16:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:55:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:55:32 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:55:35 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:55:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:55:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:56:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:56:07 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:56:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:56:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:56:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:56:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:56:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:56:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:57:01 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:57:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:57:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:57:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:57:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:57:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:57:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:57:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:58:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:58:07 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:58:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:58:22 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:58:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:58:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:58:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:58:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:59:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:59:07 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:59:22 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:59:26 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:59:43 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:59:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 11:59:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 12:00:01 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 12:00:15 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 12:00:18 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 12:00:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 12:00:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 12:00:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 12:00:50 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 12:01:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","23/11/26 12:01:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 37.7 MiB\n","[Stage 3248:===============================================>   (889 + 59) / 948]\r"]}],"source":["\n","# clean dataframe\n","def get_model_specs(df,labelcol,inputcol,finfeaturecol,model_train):\n","    # model_dict\n","    model_lst = {}\n","    \n","    # model_spec\n","    model_spec = {}\n","    \n","    # encode the reddit_data_df into subreddits one-hot encoding df\n","    tags_lst, encoded_df = encode_by_tags(df,labelcol)\n","    \n","    # train the models by different tags\n","    # 更改这个标记\n","    \n","    for tag in tags_lst[30:40]:\n","        if tag in trained_tags:\n","            continue\n","    \n","        f1_evaluator = MulticlassClassificationEvaluator(labelCol=tag, predictionCol=\"prediction\", metricName=\"f1\")\n","        accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=tag, predictionCol=\"prediction\", metricName=\"accuracy\")\n","\n","        # use clean_df to clean up special characters and spaces\n","        df_cleaned = clean_df(encoded_df,inputcol)\n","\n","        # train, test, val random split dataset\n","        train_df, test_df, val_df = train_test_val_split(df_cleaned)\n","        df_cleaned.select(tag)\n","\n","        #Preprocess the dataframes\n","        preprocess = preprocess_pipeline(tag,inputcol,finfeaturecol)\n","        preprocess = preprocess.fit(train_df)\n","        train_processed = preprocess.transform(train_df).select(finfeaturecol, tag)\n","        test_processed = preprocess.transform(test_df).select(finfeaturecol, tag)\n","        validation_processed = preprocess.transform(val_df).select(finfeaturecol, tag)\n","\n","        # find the best model by training through different hyperparameters of SVM\n","        train_processed.groupBy(tag).count().show()\n","        \n","#         print(\"start predicting best model\")\n","#         best_model = find_best_svm_hyperparameters(train_processed,finfeaturecol,tag)\n","#         print(\"done predicting best model\")\n","\n","        svm, pipeline = model_train(finfeaturecol,tag)\n","         # Predict the model\n","        model = pipeline.fit(train_processed)\n","        predictions = model.transform(test_processed)\n","        val_predictions = model.transform(validation_processed)\n","\n","        # get the acc and f1-score\n","        f1_score = f1_evaluator.evaluate(predictions)\n","        accuracy = accuracy_evaluator.evaluate(predictions)\n","        val_accuracy = accuracy_evaluator.evaluate(val_predictions)\n","        \n","        #save model and model spec\n","        model_lst[tag] = model\n","        model_spec[tag] = {\"f1_score\": f1_score, \"accuracy\": accuracy, \"val_accuracy\": val_accuracy}\n","        \n","        # 添加到 model 文件夹\n","        model.write().overwrite().save(\"gs://msca-bdp-student-gcs/Group2_Final_Project/model\"+ tag +\"_model\")\n","    \n","    return model_lst, model_spec\n","\n","# Transform the data\n","labelcol = \"subreddit_name_prefixed\"\n","inputcol = \"body\"\n","finfeaturecol = \"final_features\"\n","model_lst, model_spec = get_model_specs(reddit_data_df,labelcol,inputcol,finfeaturecol, model_training_pipeline)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"da1ae12e-f06b-4130-8440-91167f2ee6f9","metadata":{},"outputs":[],"source":["print(model_spec)"]},{"cell_type":"code","execution_count":null,"id":"6a4ea4c2-b66a-4cc7-89b7-2a855a24da0d","metadata":{},"outputs":[],"source":["\n","# convert the model into dataframe for better visualization\n","model_data = [(tag, specs[\"f1_score\"], specs[\"accuracy\"],specs[\"val_accuracy\"]) for tag, specs in model_spec.items()]\n","\n","schema = StructType([\n","    StructField(\"model\", StringType(), True),\n","    StructField(\"f1_score\", FloatType(), True),\n","    StructField(\"accuracy\", FloatType(), True),\n","    StructField(\"val_accuracy\", FloatType(), True)\n","])\n","\n","# Create DataFrame\n","model_df = spark.createDataFrame(reddit_data_df, schema)\n","\n","model_df.show()"]},{"cell_type":"code","execution_count":null,"id":"ebb04b66-a717-4be0-b597-8d562a48d69c","metadata":{},"outputs":[],"source":["# save all the models\n","# for key in model_lst:\n","#     model_lst[key].write().overwrite().save(\"msca-bdp-student-gcs/Group2_Final_Project/model\"+ key +\"_model\")"]},{"cell_type":"code","execution_count":null,"id":"ffe5bc14-7e21-44fd-a6e6-f641caeb7c9d","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"27343c3f-d85e-4fae-bc4a-956b8340e3b7","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":5}
