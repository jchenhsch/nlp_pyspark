{"cells":[{"cell_type":"code","execution_count":4,"id":"71bc1484-acde-460d-84b5-71053f282877","metadata":{},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.types import StringType, DateType, FloatType, IntegerType, TimestampType, ArrayType, StructType, StructField\n","from pyspark.sql.functions import from_unixtime, sum, rank,lag, explode, expr,spark_partition_id, to_date, coalesce, lit, to_timestamp, col, month, concat, count, max, when, dayofweek, datediff,dense_rank, desc, date_format\n","import pyspark.sql.functions as F\n","from pyspark.sql.window import Window\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import re\n","from pyspark.ml.feature import Tokenizer, StopWordsRemover, Word2Vec,HashingTF,IDF, CountVectorizer,VectorAssembler\n","from pyspark.sql.functions import udf\n","from pyspark.ml.feature import Tokenizer, CountVectorizer, IDF\n","from pyspark.ml import Pipeline,PipelineModel\n","from sparknlp.base import DocumentAssembler, Finisher\n","from sparknlp.annotator import LemmatizerModel\n","from pyspark.ml.classification import LinearSVC, LogisticRegression\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator,BinaryClassificationEvaluator\n","from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n","import sparknlp\n","import warnings"]},{"cell_type":"code","execution_count":5,"id":"8ece1179-fc2a-4b12-93b6-0910fa154652","metadata":{},"outputs":[],"source":["# remove all warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":6,"id":"b08a2971-7bc1-4bd6-ab96-6837bd3569d3","metadata":{},"outputs":[],"source":["# spark = SparkSession.builder.appName('SparkBasics').getOrCreate()\n","spark = SparkSession.builder.appName('ml').getOrCreate()\n","\n","# Get the context of the Pyspark environment\n","spark.sparkContext.getConf().getAll()\n","# Store spark context as a variable\n","sc = spark.sparkContext"]},{"cell_type":"code","execution_count":7,"id":"435cc3d3-c010-4441-8f3f-b7065e43971a","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["root\n"," |-- archived: string (nullable = true)\n"," |-- author: string (nullable = true)\n"," |-- author_fullname: string (nullable = true)\n"," |-- body: string (nullable = true)\n"," |-- comment_type: string (nullable = true)\n"," |-- controversiality: string (nullable = true)\n"," |-- created_utc: string (nullable = true)\n"," |-- edited: string (nullable = true)\n"," |-- gilded: string (nullable = true)\n"," |-- id: string (nullable = true)\n"," |-- link_id: string (nullable = true)\n"," |-- locked: string (nullable = true)\n"," |-- name: string (nullable = true)\n"," |-- parent_id: string (nullable = true)\n"," |-- permalink: string (nullable = true)\n"," |-- retrieved_on: string (nullable = true)\n"," |-- score: string (nullable = true)\n"," |-- subreddit_id: string (nullable = true)\n"," |-- subreddit_name_prefixed: string (nullable = true)\n"," |-- subreddit_type: string (nullable = true)\n"," |-- total_awards_received: string (nullable = true)\n","\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 2:==============>                                            (1 + 3) / 4]\r"]},{"name":"stdout","output_type":"stream","text":["+-----------------------+\n","|subreddit_name_prefixed|\n","+-----------------------+\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","|                  r/DIY|\n","+-----------------------+\n","only showing top 20 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["reddit_data_df = spark.read.parquet(\"gs://msca-bdp-student-gcs/Group2_Final_Project/reddit_data/\",header=True, inferSchema=True)\n","reddit_data_df = reddit_data_df.dropna()\n","\n","reddit_data_df.printSchema()\n","reddit_data_df.select(col(\"subreddit_name_prefixed\")).show()"]},{"cell_type":"code","execution_count":8,"id":"97ef648a-d06e-4b18-b26c-66446439981e","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-----------------------+--------+\n","|subreddit_name_prefixed|count(1)|\n","+-----------------------+--------+\n","|                  r/DIY|     100|\n","|                r/Games|     100|\n","+-----------------------+--------+\n","\n"]}],"source":["# for debug only, slice datasets\n","# Assuming df is your original DataFrame\n","games_df = reddit_data_df.filter(col(\"subreddit_name_prefixed\") == \"r/Games\").limit(100)\n","diy_df = reddit_data_df.filter(col(\"subreddit_name_prefixed\") == \"r/DIY\").limit(100)\n","\n","# If you need to combine these two DataFrames\n","combined_df = games_df.union(diy_df)\n","\n","# Show the result\n","combined_df.groupBy(col(\"subreddit_name_prefixed\")).agg(count(\"*\")).show()\n"]},{"cell_type":"code","execution_count":9,"id":"bde6de00-0f3c-42d4-8259-15fb566565c8","metadata":{},"outputs":[],"source":["# Tokenize and stop word removal\n","def clean_text(text):\n","    # Deal with component words\n","    re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', text)\n","    # Convert to lowercase\n","    text = text.lower()\n","    # Remove Http / Https links in the text\n","    text = re.sub(r'http\\S+', '', text)\n","    text = re.sub(r'https\\S+', '', text)\n","    # Remove special characters and numbers\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","    # Handling repeated characters (more than 2)\n","    text = re.sub(r'(.)\\1+', r'\\1\\1', text)\n","    # Remove extra spaces\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text"]},{"cell_type":"code","execution_count":10,"id":"8055980a-8efb-4067-b4e7-99147868f489","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 16:=====================================================>(947 + 1) / 948]\r"]},{"name":"stdout","output_type":"stream","text":["+-----------------------+------------+\n","|subreddit_name_prefixed|reddit_count|\n","+-----------------------+------------+\n","|                  r/DIY|         100|\n","|                r/Games|         100|\n","+-----------------------+------------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# for debug only -- slice count datasets\n","reddit_count_df = combined_df.groupby(col(\"subreddit_name_prefixed\")).agg(count(\"*\").alias(\"reddit_count\")).orderBy(\"reddit_count\", ascending=False)\n","reddit_count_df.show()"]},{"cell_type":"code","execution_count":11,"id":"ff45b67c-6fcd-4f86-b08b-11b0500de368","metadata":{},"outputs":[],"source":["def encode_by_tags(df,labelcol):\n","    \n","    tags_lst = [row[labelcol] for row in df.select(col(labelcol)).distinct().collect()]\n","    df_res = df\n","    for tag in tags_lst:\n","        print(tag)\n","        df_res= df_res.withColumn(tag, when(col(labelcol) == tag, 1).otherwise(0))\n","\n","    return tags_lst, df_res"]},{"cell_type":"code","execution_count":12,"id":"1d1fe0c7-0e27-4bc0-b94a-382897834039","metadata":{},"outputs":[],"source":["\n","def clean_df(df, inputcol):\n","    clean_text_udf = udf(clean_text, StringType())\n","    df_cleaned = df.withColumn(inputcol, clean_text_udf(df[inputcol]))\n","    return df_cleaned\n","\n","def train_test_val_split(df, train_prob = 0.7, test_prob=0.2, val_prob= 0.1):\n","    train_df, test_df, validation_df = df.randomSplit([train_prob, test_prob, val_prob])\n","    return train_df, test_df, validation_df\n","    \n","\n","def preprocess_pipeline(labelcol, inputcol,finfeaturecol):\n","    # tokenize the comments into words\n","    tokenizer = Tokenizer(inputCol=inputcol, outputCol=\"token\")\n","    \n","    # remove stop words\n","    remover = StopWordsRemover(inputCol=\"token\", outputCol=\"filtered_token\")\n","    \n","    # vecotorize the words\n","    vectorizer = CountVectorizer(inputCol=\"filtered_token\", outputCol=\"features\")\n","    idf = IDF(inputCol=\"features\", outputCol=\"tfidf_features\")\n","    \n","    # assemble all features into 1 column\n","    assembler = VectorAssembler(inputCols=[\"features\",\"tfidf_features\"], outputCol=finfeaturecol)\n","\n","    # Create the preprocessing piplines for the tweets\n","    pipeline = Pipeline().setStages([\n","        tokenizer,\n","        remover,\n","        vectorizer,\n","        idf,\n","        assembler\n","    ])\n","    return pipeline\n","\n","def model_training_pipeline(featurecol, labelcol):\n","    svm = LinearSVC(labelCol=labelcol, featuresCol=featurecol)\n","    pipeline = Pipeline(stages=[svm])\n","    return svm,pipeline"]},{"cell_type":"code","execution_count":13,"id":"a5620ce3-3644-4e3c-8653-31a3e6e46369","metadata":{},"outputs":[],"source":["def find_best_svm_hyperparameters(df,featurecol,labelCol):\n","    \n","    svm,pipeline = model_training_pipeline(featurecol, labelcol)\n","    # Set up the parameter grid\n","    paramGrid = ParamGridBuilder() \\\n","        .addGrid(svm.maxIter, [10, 100]) \\\n","        .addGrid(svm.regParam, [0.01, 0.1, 1.0]) \\\n","        .build()\n","\n","    # Evaluator\n","    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=labelCol)\n","    df.select(col(featurecol)).show()\n","\n","    # Cross Validator\n","    crossval = CrossValidator(estimator=svm,\n","                              estimatorParamMaps=paramGrid,\n","                              evaluator=evaluator,\n","                              numFolds=3)\n","\n","    # Run cross-validation and choose the best model\n","    print(\"start fitting\")\n","    cvModel = crossval.fit(df)\n","    print(cvModel)\n","\n","    return cvModel.bestModel"]},{"cell_type":"code","execution_count":14,"id":"9e809b26-915f-42b6-8b04-64e028f0869e","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["r/DIY\n","r/Games\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+--------------------+-----+\n","|      final_features|r/DIY|\n","+--------------------+-----+\n","|(2976,[0,1,3,4,9,...|    0|\n","|(2976,[1,20,21,28...|    0|\n","|(2976,[11,28,309,...|    0|\n","|(2976,[147,905,10...|    0|\n","|(2976,[115,134,14...|    0|\n","|(2976,[1,10,12,14...|    0|\n","|(2976,[10,51,255,...|    0|\n","|(2976,[7,30,39,66...|    0|\n","|(2976,[1,8,23,122...|    0|\n","|(2976,[7,46,99,12...|    0|\n","|(2976,[14,46,1010...|    0|\n","|(2976,[1,2,7,8,10...|    0|\n","|(2976,[1,7,10,28,...|    0|\n","|(2976,[1,7,20,28,...|    0|\n","|(2976,[23,27,44,4...|    0|\n","|(2976,[227,244,35...|    0|\n","|(2976,[1,14,21,34...|    0|\n","|(2976,[7,21,88,11...|    0|\n","|(2976,[21,23,28,3...|    0|\n","|(2976,[1082,2570]...|    0|\n","+--------------------+-----+\n","only showing top 20 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["23/11/28 04:28:19 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n","23/11/28 04:28:19 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+--------------------+-------+\n","|      final_features|r/Games|\n","+--------------------+-------+\n","|(3176,[3,16,32,34...|      1|\n","|(3176,[3,12,22,38...|      1|\n","|(3176,[13,94,150,...|      1|\n","|(3176,[0,2,3,4,5,...|      1|\n","|(3176,[3,23,44,51...|      1|\n","|(3176,[21,23,297,...|      1|\n","|(3176,[118,289,17...|      1|\n","|(3176,[3,10,12,28...|      1|\n","|(3176,[3,13,106,1...|      1|\n","|(3176,[3,23,231,4...|      1|\n","|(3176,[463,2051],...|      1|\n","|(3176,[21,63,135,...|      1|\n","|(3176,[1,175,180,...|      1|\n","|(3176,[28,100,102...|      1|\n","|(3176,[12,215,289...|      1|\n","|(3176,[1,3,7,10,1...|      1|\n","|(3176,[1,3,10,23,...|      1|\n","|(3176,[1,3,23,51,...|      1|\n","|(3176,[0,26,40,69...|      1|\n","|(3176,[1,3,224,35...|      1|\n","+--------------------+-------+\n","only showing top 20 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["\n","# clean dataframe\n","def get_model_specs(df,labelcol,inputcol,finfeaturecol,model_train):\n","    # model_dict\n","    model_lst = {}\n","    \n","    # model_spec\n","    model_spec = {}\n","    \n","    # encode the reddit_data_df into subreddits one-hot encoding df\n","    tags_lst, encoded_df = encode_by_tags(df,labelcol)\n","    \n","    # train the models by different tags\n","    # 更改这个标记\n","    for tag in tags_lst:\n","    \n","        f1_evaluator = MulticlassClassificationEvaluator(labelCol=tag, predictionCol=\"prediction\", metricName=\"f1\")\n","        accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=tag, predictionCol=\"prediction\", metricName=\"accuracy\")\n","\n","        # use clean_df to clean up special characters and spaces\n","        df_cleaned = clean_df(encoded_df,inputcol)\n","\n","        # train, test, val random split dataset\n","        train_df, test_df, val_df = train_test_val_split(df_cleaned)\n","        df_cleaned.select(tag)\n","\n","        #Preprocess the dataframes\n","        preprocess = preprocess_pipeline(tag,inputcol,finfeaturecol)\n","        preprocess = preprocess.fit(train_df)\n","        train_processed = preprocess.transform(train_df).select(finfeaturecol, tag)\n","        test_processed = preprocess.transform(test_df).select(finfeaturecol, tag)\n","        validation_processed = preprocess.transform(val_df).select(finfeaturecol, tag)\n","        \n","        train_processed.show()\n","        \n","        svm, pipeline = model_train(finfeaturecol,tag)\n","         # Predict the model\n","        model = pipeline.fit(train_processed)\n","        predictions = model.transform(test_processed)\n","        val_predictions = model.transform(validation_processed)\n","\n","        # get the acc and f1-score\n","        f1_score = f1_evaluator.evaluate(predictions)\n","        accuracy = accuracy_evaluator.evaluate(predictions)\n","        val_accuracy = accuracy_evaluator.evaluate(val_predictions)\n","        \n","        #save model and model spec\n","        model_lst[tag] = model\n","        model_spec[tag] = {\"f1_score\": f1_score, \"accuracy\": accuracy, \"val_accuracy\": val_accuracy}\n","        \n","#         # 添加到 model 文件夹\n","#         model.write().overwrite().save(\"gs://msca-bdp-student-gcs/Group2_Final_Project/test\")\n","    \n","    return model_lst, model_spec\n","\n","# Transform the data\n","labelcol = \"subreddit_name_prefixed\"\n","inputcol = \"body\"\n","finfeaturecol = \"final_features\"\n","model_lst, model_spec = get_model_specs(combined_df,labelcol,inputcol,finfeaturecol, model_training_pipeline)\n","\n"]},{"cell_type":"code","execution_count":15,"id":"da1ae12e-f06b-4130-8440-91167f2ee6f9","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'r/DIY': {'f1_score': 0.6583416583416584, 'accuracy': 0.717948717948718, 'val_accuracy': 0.6666666666666666}, 'r/Games': {'f1_score': 0.7114431239388794, 'accuracy': 0.82, 'val_accuracy': 0.8333333333333334}}\n"]}],"source":["print(model_spec)"]},{"cell_type":"code","execution_count":16,"id":"6a4ea4c2-b66a-4cc7-89b7-2a855a24da0d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 1286:=============================================>        (17 + 3) / 20]\r"]},{"name":"stdout","output_type":"stream","text":["+-------+----------+----------+------------+\n","|  model|  f1_score|  accuracy|val_accuracy|\n","+-------+----------+----------+------------+\n","|  r/DIY|0.65834165|0.71794873|   0.6666667|\n","|r/Games| 0.7114431|      0.82|   0.8333333|\n","+-------+----------+----------+------------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["\n","# convert the model into dataframe for better visualization\n","model_data = [(tag, specs[\"f1_score\"], specs[\"accuracy\"],specs[\"val_accuracy\"]) for tag, specs in model_spec.items()]\n","\n","schema = StructType([\n","    StructField(\"model\", StringType(), True),\n","    StructField(\"f1_score\", FloatType(), True),\n","    StructField(\"accuracy\", FloatType(), True),\n","    StructField(\"val_accuracy\", FloatType(), True)\n","])\n","\n","# Create DataFrame\n","model_df = spark.createDataFrame(model_data, schema)\n","\n","model_df.show()"]},{"cell_type":"code","execution_count":17,"id":"ebb04b66-a717-4be0-b597-8d562a48d69c","metadata":{},"outputs":[],"source":["# save all the models\n","# for key in model_lst:\n","#     model_lst[key].write().overwrite().save(\"msca-bdp-student-gcs/Group2_Final_Project/model\"+ key +\"_model\")"]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":5}
